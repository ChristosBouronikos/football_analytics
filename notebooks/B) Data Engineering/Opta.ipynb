{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering of Opta Data\n",
    "##### Notebook to engineer JSON data from [Opta](https://www.optasports.com/).\n",
    "\n",
    "### By [Edd Webster](https://www.twitter.com/eddwebster)\n",
    "Last updated: 01/09/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../../img/opta_sports_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](#section5) to jump straight to the Exploratory Data Analysis section and skip the [Task Brief](#section2), [Data Sources](#section3), and [Data Engineering](#section4) sections. Or click [here](#section6) to jump straight to the Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='sectionintro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='import_libraries'>Introduction</a>\n",
    "This notebook scrapes data for player valuations using [Beautifulsoup](https://pypi.org/project/beautifulsoup4/) from [TransferMarkt](https://www.transfermarkt.co.uk/) using [pandas](http://pandas.pydata.org/) for data maniuplation through DataFrames, [Beautifulsoup](https://pypi.org/project/beautifulsoup4/) for webscraping.\n",
    "\n",
    "For more information about this notebook and the author, I'm available through all the following channels:\n",
    "*    [eddwebster.com](https://www.eddwebster.com/),\n",
    "*    edd.j.webster@gmail.com,\n",
    "*    [@eddwebster](https://www.twitter.com/eddwebster),\n",
    "*    [LinkedIn.com/in/eddwebster](https://www.linkedin.com/in/eddwebster/),\n",
    "*    [GitHub/eddwebster](https://github.com/eddwebster/),\n",
    "*    [Kaggle.com/eddwebster](https://www.kaggle.com/eddwebster), and\n",
    "*    [HackerRank.com/eddwebster](https://www.hackerrank.com/eddwebster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accompanying GitHub repository for this notebook can be found [here](https://github.com/eddwebster/fifa-league) and a static version of this notebook can be found [here](https://nbviewer.jupyter.org/github/eddwebster/fifa-league/blob/master/FIFA%2020%20Fantasy%20Football%20League%20using%20TransferMarkt%20Player%20Valuations.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='sectioncontents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='notebook_contents'>Notebook Contents</a>\n",
    "1.    [Notebook Dependencies](#section1)<br>\n",
    "2.    [Project Brief](#section2)<br>\n",
    "3.    [Data Sources](#section3)<br>\n",
    "      1.    [Introduction](#section3.1)<br>\n",
    "      2.    [Data Dictionary](#section3.2)<br>\n",
    "      3.    [Creating the DataFrame](#section3.3)<br>\n",
    "      4.    [Initial Data Handling](#section3.4)<br>\n",
    "      5.    [Export the Raw DataFrame](#section3.5)<br>         \n",
    "4.    [Data Engineering](#section4)<br>\n",
    "      1.    [Introduction](#section4.1)<br>\n",
    "      2.    [Columns of Interest](#section4.2)<br>\n",
    "      3.    [String Cleaning](#section4.3)<br>\n",
    "      4.    [Converting Data Types](#section4.4)<br>\n",
    "      5.    [Export the Engineered DataFrame](#section4.5)<br>\n",
    "5.    [Exploratory Data Analysis (EDA)](#section5)<br>\n",
    "      1.    [...](#section5.1)<br>\n",
    "      2.    [...](#section5.2)<br>\n",
    "      3.    [...](#section5.3)<br>\n",
    "6.    [Summary](#section6)<br>\n",
    "7.    [Next Steps](#section7)<br>\n",
    "8.    [Bibliography](#section8)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section1'>1. Notebook Dependencies</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written using [Python 3](https://docs.python.org/3.7/) and requires the following libraries:\n",
    "*    [`Jupyter notebooks`](https://jupyter.org/) for this notebook environment with which this project is presented;\n",
    "*    [`NumPy`](http://www.numpy.org/) for multidimensional array computing;\n",
    "*    [`pandas`](http://pandas.pydata.org/) for data analysis and manipulation;\n",
    "*    `tqdm` for a clean progress bar;\n",
    "*    `requests` for executing HTTP requests;\n",
    "*    [`Beautifulsoup`](https://pypi.org/project/beautifulsoup4/) for web scraping; and\n",
    "*    [`matplotlib`](https://matplotlib.org/contents.html?v=20200411155018) for data visualisations;\n",
    "\n",
    "All packages used for this notebook except for BeautifulSoup can be obtained by downloading and installing the [Conda](https://anaconda.org/anaconda/conda) distribution, available on all platforms (Windows, Linux and Mac OSX). Step-by-step guides on how to install Anaconda can be found for Windows [here](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) and Mac [here](https://medium.com/@GalarnykMichael/install-python-on-mac-anaconda-ccd9f2014072), as well as in the Anaconda documentation itself [here](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# Python â‰¥3.5 (ideally)\n",
    "import platform\n",
    "import sys, getopt\n",
    "assert sys.version_info >= (3, 5)\n",
    "import csv\n",
    "\n",
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Math Operations\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd    # version 1.0.3\n",
    "import os    #  used to read the csv filenames\n",
    "import re\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Reading directories\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Working with JSON\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Fuzzy Matching - Record Linkage\n",
    "import recordlinkage\n",
    "import jellyfish\n",
    "import numexpr as ne\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno as msno    # visually display missing data\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm    # a clean progress bar library\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import Image, YouTubeVideo\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from utils import *\n",
    "from plot_utils import *\n",
    "from metrics import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6\n",
      "NumPy: 1.18.1\n",
      "pandas: 1.0.1\n",
      "matplotlib: 3.1.3\n",
      "Seaborn: 0.10.0\n"
     ]
    }
   ],
   "source": [
    "# Python / module versions used here for reference\n",
    "print('Python: {}'.format(platform.python_version()))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(mpl.__version__))\n",
    "print('Seaborn: {}'.format(sns.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define today's date\n",
    "today = datetime.datetime.now().strftime('%d/%m/%Y').replace('/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial paths to subfolders\n",
    "base_dir = os.path.join('..', '..', )\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "data_dir_fbref = os.path.join(base_dir, 'data', 'fbref')\n",
    "data_dir_tm = os.path.join(base_dir, 'data', 'tm')\n",
    "img_dir = os.path.join(base_dir, 'img')\n",
    "fig_dir = os.path.join(base_dir, 'img', 'fig')\n",
    "video_dir = os.path.join(base_dir, 'video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section2'>2. Project Brief</a>\n",
    "This Jupyter notebook explores how to scrape football data from [TransferMarkt](https://www.transfermarkt.co.uk/), using [pandas](http://pandas.pydata.org/) for data maniuplation through DataFrames and [Beautifulsoup](https://pypi.org/project/beautifulsoup4/) for webscraping.\n",
    "\n",
    "The data of player values produced in this notebook is exported to CSV. This data can be further analysed in Python, joined to other datasets, or explored using Tableau, PowerBI, Microsoft Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section3'>3. Data Sources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.1'>3.1. Introduction</a>\n",
    "[TransferMarkt](https://www.transfermarkt.co.uk/) is a German-based website owned by [Axel Springer](https://www.axelspringer.com/en/) and is the leading website for the football transfer market. The website posts football related data, including: scores and results, football news, transfer rumours, and most usefully for us - calculated estimates ofthe market values for teams and individual players.\n",
    "\n",
    "To read more about how these estimations are made, [Beyond crowd judgments: Data-driven estimation of market value in association football](https://www.sciencedirect.com/science/article/pii/S0377221717304332) by Oliver MÃ¼llera, Alexander Simons, and Markus Weinmann does an excellent job of explaining how the estimations are made and their level of accuracy.\n",
    "\n",
    "Before conducting our EDA, the data needs to be imported as a DataFrame in the Data Sources section [Section 3](#section3) and Cleaned in the Data Engineering section [Section 4](#section4).\n",
    "\n",
    "We'll be using the [pandas](http://pandas.pydata.org/) library to import our data to this workbook as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.2'>3.2. Data Dictionaries</a>\n",
    "The [TransferMarkt](https://www.transfermarkt.co.uk/) dataset has six features (columns) with the following definitions and data types:\n",
    "\n",
    "| Feature     | Data type    |\n",
    "|------|-----|\n",
    "| `position_number`    | object     |\n",
    "| `position_description`    | object     |\n",
    "| `name`    | object     |\n",
    "| `dob`    | object     |\n",
    "| `nationality`    | object     |\n",
    "| `value`    | object     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_download() # automatic download from public figshare repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 17825793 (char 17825792)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0b42b2e7eff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatch_id2match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_id2events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_id2player\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompetition_id2competition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam_id2team\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_public_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Programming/Projects by Type/Football/Plymouth/notebooks/B) Data Engineering/utils.py\u001b[0m in \u001b[0;36mload_public_dataset\u001b[0;34m(data_folder, tournament)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/events/events_%s.json'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mtournament\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/matches/matches_%s.json'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mtournament\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 17825793 (char 17825792)"
     ]
    }
   ],
   "source": [
    "match_id2match, match_id2events, player_id2player, competition_id2competition, team_id2team = load_public_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.3'>3.3. Creating the DataFrame - scraping the data</a>\n",
    "Before scraping data from [TransferMarkt](https://www.transfermarkt.co.uk/), we need to look at the top five leagues that we wish to scrape.\n",
    "\n",
    "The web scraper for [TransferMarkt](https://www.transfermarkt.co.uk/) is made up of two parts:\n",
    "1.    In the first part, the scraper takes the webpages for each of the individual leagues  e.g. The Premier League, and extract the hyperlinks to the pages of all the individual teams in the league table.\n",
    "2.    In the second part the script, the webscraper uses the list of invidual teams hyperlinks collected in part 1 to then collect the hyperlinks for each of the players for those teams. From this, the scraper can then extract the information we need for each of these players.\n",
    "\n",
    "This information collected for all the players is converted to a [pandas](http://pandas.pydata.org/) DataFrame from which we can view and manipulate the data.\n",
    "\n",
    "An example webpage for a football league is the following: https://www.transfermarkt.co.uk/jumplist/startseite/wettbewerb/GB1/plus/?saison_id=2019. As we can see, between the subdirectory path of `'/wettbewerb/'` and the `'/plus/'`, there is a 3 or 4 digit code. For The Premier League, the code is GB1. \n",
    "\n",
    "In order to scrape the webpages from [TransferMarkt](https://www.transfermarkt.co.uk/), the codes of the top five leagues need to be recorded from [TransferMarkt](https://www.transfermarkt.co.uk/), which are the following:\n",
    "\n",
    "| League Name on FIFA    | Country    | Corresponding [TransferMarkt](https://www.transfermarkt.co.uk/) League Code    |\n",
    "|------|-----|-----|\n",
    "| LaLiga Santander    | Spain    | ES1    |\n",
    "| Ligue 1 Conforama    | France    | FR1    |\n",
    "| Premier League    | England    | GB1    |\n",
    "| Serie A TIM    | Italy    | IT1    |\n",
    "| Bundesliga    | Germany    | L1    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of leagues by code for which we want to scrape player data - Big 5 European leagues\n",
    "lst_leagues = ['ES1', 'FR1', 'GB1', 'IT1', 'L1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign season by year to season variable e.g. 2014/15 season = 2014\n",
    "season = '2020'    # 2020/21 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this script to scrape latest version of this data from TransferMarkt\n",
    "\n",
    "## Start timer\n",
    "tic = datetime.datetime.now()\n",
    "\n",
    "\n",
    "## Scrape TransferMarkt data\n",
    "def main(url):\n",
    "    with requests.Session() as req:\n",
    "        links = []\n",
    "        for league in lst_leagues:\n",
    "            print(f'Fetching Links from {league}')\n",
    "            r = req.get(url.format(league), headers=headers)\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            link = [f\"{url[:31]}{item.next_element.get('href')}\" for item in soup.findAll(\n",
    "                \"td\", class_=\"hauptlink no-border-links hide-for-small hide-for-pad\")]\n",
    "            links.extend(link)\n",
    "\n",
    "        print(f'Collected {len(links)} Links')\n",
    "        goals = []\n",
    "        for num, link in enumerate(links):\n",
    "            print(f\"Extracting Page# {num +1}\")\n",
    "            r = req.get(link, headers=headers)\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            target = soup.find(\"table\", class_=\"items\")\n",
    "            pn = [pn.text for pn in target.select(\"div.rn_nummer\")]\n",
    "            pos = [pos.text for pos in target.findAll(\"td\", class_=False)]\n",
    "            name = [name.text for name in target.select(\"td.hide\")]\n",
    "            dob = [date.find_next(\n",
    "                \"td\").text for date in target.select(\"td.hide\")]\n",
    "            nat = [\" / \".join([a.get(\"alt\") for a in nat.find_all_next(\"td\")[1] if a.get(\"alt\")]) for nat in target.findAll(\n",
    "                \"td\", itemprop=\"athlete\")]\n",
    "            val = [val.get_text(strip=True)\n",
    "                   for val in target.select('td.rechts.hauptlink')]\n",
    "            goal = zip(pn, pos, name, dob, nat, val)\n",
    "            df = pd.DataFrame(goal, columns=[\n",
    "                              'position_number', 'position_description', 'name', 'dob', 'nationality', 'value'])\n",
    "            goals.append(df)\n",
    "\n",
    "        new = pd.concat(goals)\n",
    "        new.to_csv(data_dir_tm + '/raw/' + f'players_big5_2021_raw_{today}.csv', index=None, header=True)\n",
    "\n",
    "main(\"https://www.transfermarkt.co.uk/jumplist/startseite/wettbewerb/{}/plus/?saison_id=2020\")\n",
    "#main(f'https://www.transfermarkt.co.uk/jumplist/startseite/wettbewerb/{}/plus/?saison_id={season}')\n",
    "\n",
    "\n",
    "## End timer\n",
    "toc = datetime.datetime.now()\n",
    "\n",
    "\n",
    "## Calculate time take\n",
    "total_time = (toc-tic).total_seconds()\n",
    "print(f'Time taken to scrape {len(links)} pages for the Big 5 leagues is: {total_time/60:0.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data as a pandas DataFrame, df_tm_players_big5_2021_raw\n",
    "\n",
    "## Look for most recent CSV file\n",
    "list_of_files = glob.glob(data_dir_tm + '/raw/*')    # * means all if need specific format then *.csv\n",
    "filepath_latest_tm = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "## Load in most recently parsed CSV file\n",
    "df_tm_player_top5_2021_raw = pd.read_csv(filepath_latest_tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.3'>3.4. Preliminary Data Handling</a>\n",
    "Let's quality of the dataset by looking first and last rows in pandas using the [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the raw DataFrame, df_tm_player_top5_2021_raw\n",
    "df_tm_player_top5_2021_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of the raw DataFrame, df_tm_player_top5_2021_raw\n",
    "df_tm_player_top5_2021_raw.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the raw DataFrame, df_tm_player_top5_2021_raw\n",
    "print(df_tm_player_top5_2021_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the raw DataFrame, df_tm_player_top5_2021_raw\n",
    "print(df_tm_player_top5_2021_raw.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has six features (columns). Full details of these attributes can be found in the [Data Dictionary](section3.3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of the features of the raw DataFrame, df_tm_player_top5_2021_raw\n",
    "df_tm_player_top5_2021_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All six of the columns have the object data type. Full details of these attributes and their data types can be found in the [Data Dictionary](section3.3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_tm_player_top5_2021_raw\n",
    "df_tm_player_top5_12021_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the raw DataFrame, df_tm_player_top5_2021_raw, showing some summary statistics for each numberical column in the DataFrame\n",
    "df_tm_player_top5_2021_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_tm_player_top5_2021_raw\n",
    "msno.matrix(df_tm_player_top5_2021_raw, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "tm_null_value_stats = df_tm_player_top5_2021_raw.isnull().sum(axis=0)\n",
    "tm_null_value_stats[tm_null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows us very quickly that there a few missing values in the `value` column, but otherwise the dataset is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section4'>4. Data Engineering</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.1'>4.1. Introduction</a>\n",
    "Before we answer the questions in the brief through [Exploratory Data Analysis (EDA)](#section5), we'll first need to clean and wrangle the datasets to a form that meet our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.2'>4.2. Assign Raw DataFrames to New Engineered DataFrames</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Raw DataFrame to new Engineered DataFrame\n",
    "df_tm_player_top5_2021 = df_tm_player_top5_2021_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.2'>4.2. String Cleaning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tm_player_top5_2021['name_lower'] = df_tm_player_top5_2021['name'].str.normalize('NFKD')\\\n",
    "                                                                     .str.encode('ascii', errors='ignore')\\\n",
    "                                                                     .str.decode('utf-8')\\\n",
    "                                                                     .str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Name Lower\n",
    "df_tm_player_top5_2021['firstname_lower'] = df_tm_player_top5_2021['name_lower'].str.rsplit(' ', 0).str[0]\n",
    "\n",
    "# Last Name Lower\n",
    "df_tm_player_top5_2021['lastname_lower'] = df_tm_player_top5_2021['name_lower'].str.rsplit(' ', 1).str[-1]\n",
    "\n",
    "# First Initial Lower\n",
    "df_tm_player_top5_2021['firstinitial_lower'] = df_tm_player_top5_2021['name_lower'].astype(str).str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DoB and Age\n",
    "The `dob` column is messy and contains both the date of birth as a string and also the age in brackets.\n",
    "\n",
    "This string cleaning consists of two parts, firstly, to split this apart into their seperate components. However, once the `age` column is created, we will replaced this by determining the current age using the Python [datetime](https://docs.python.org/3/library/datetime.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoB string cleaning to create birth_date and age columns\n",
    "df_tm_player_top5_2021[['birth_date', 'age']] = df_tm_player_top5_2021['dob'].str.extract(r'(.+) \\((\\d+)\\)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nationality\n",
    "For the nationality, some of the players have duel nationality.\n",
    "\n",
    "For example, [Claudio Pizarro](https://www.transfermarkt.co.uk/claudio-pizarro/profil/spieler/532) is a Peruvian-born player who has has made 85 appearances for Peru, scoring 20 goals. However, his citizenship according to [TransferMarkt](https://www.transfermarkt.co.uk/) is 'Peru / Italy'. For our needs, we only want to know the country the player is eligible to play for, not their full heritage which from observations is always the first part of the string. We'll therefore be discarding anything after the first space in the string to form a new `playing_country` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first nationality i.e. text before the first space, ex. 'Peru / Italy'\n",
    "df_tm_player_top5_2021['playing_country'] = df_tm_player_top5_2021['nationality'].str.split(' /').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value\n",
    "The values of the players have prefixes (Â£), commas, spaces, and suffixes (m, k) that need to cleaned and replaced before converting to a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value string cleaning from shortened string value to full numerical value\n",
    "\n",
    "## Convert 'm' to '000000'\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace('m','0000')\n",
    "\n",
    "## Convert 'k' to '000'\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace('k','000')\n",
    "\n",
    "## Convert 'Th' to '000'\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace('Th','000')\n",
    "\n",
    "## Remove '.'\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace('.','')\n",
    "\n",
    "## Remove 'Â£' sign\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace('Â£','')\n",
    "\n",
    "## Remove '-'\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace('-','')\n",
    "\n",
    "## Remove 'Â¬â€ Â¬â€ '\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace('Â¬â€ Â¬â€ ','')\n",
    "\n",
    "## Remove gaps\n",
    "df_tm_player_top5_2021['value'] = df_tm_player_top5_2021['value'].str.replace(' ','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.3'>4.3. Converting Data Types</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DoB\n",
    "First we need to convert the `dob` column from the `object` data type to `datetime64[ns]`, again using the [.to_datetime()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert birth_date from string to datetime64[ns]\n",
    "df_tm_player_top5_2021['birth_date'] = pd.to_datetime(df_tm_player_top5_2021['birth_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age\n",
    "The calculated `age` column needs to be converted from a float to an integer, with all null values ignored, using to [astype()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date and time manipulation\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redetermine the age using the newly created birth_date column (after formatted to datetime data type)\n",
    "\n",
    "## Remove all not numeric values use to_numeric with parameter errors='coerce' - it replaces non numeric to NaNs\n",
    "df_tm_player_top5_2021['age'] = pd.to_numeric(df_tm_player_top5_2021['age'], errors='coerce')\n",
    "\n",
    "## Convert floats to integers and leave null values\n",
    "df_tm_player_top5_2021['age'] = np.nan_to_num(df_tm_player_top5_2021['age']).astype(int)\n",
    "\n",
    "## Calculate current age\n",
    "today = datetime.today()\n",
    "df_tm_player_top5_2021['age'] = df_tm_player_top5_2021['birth_date'].apply(lambda x: today.year - x.year - \n",
    "                                                                               ((today.month, today.day) < (x.month, x.day)) \n",
    "                                                                          )\n",
    "\n",
    "\n",
    "# df_tm_player_top5_2021['age'] = pd.to_numeric(ddf_tm_player_top5_2021['age'], downcast='signed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value\n",
    "The `value` column needs to be converted from a string to an integer using to [to_numeric()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to integer\n",
    "df_tm_player_top5_2021['value'] = pd.to_numeric(df_tm_player_top5_2021['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Position\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_tm_player_top5_2021['position_description'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_positions_tm = {\n",
    "    'Attacking Midfield': 'Midfielder',\n",
    "    'Central Midfield': 'Midfielder',\n",
    "    'Centre-Back': 'Defender',\n",
    "    'Centre-Forward': 'Forward',\n",
    "    'Defensive Midfield': 'Midfielder',\n",
    "    'Forward': 'Forward',\n",
    "    'Goalkeeper': 'Goalkeeper',\n",
    "    'Left Midfield': 'Midfielder',\n",
    "    'Left Winger': 'Forward',\n",
    "    'Left-Back': 'Defender',\n",
    "    'Right Midfield': 'Midfielder',\n",
    "    'Right Winger': 'Forward',\n",
    "    'Right-Back': 'Defender',\n",
    "    'Second Striker': 'Forward'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tm_player_top5_2021['position_description_cleaned'] = df_tm_player_top5_2021['position_description'].map(dict_positions_tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.4'>4.4. Create New Attributes</a>\n",
    "Create new attributes for birth month and birth year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tm_player_top5_2021['birth_year'] = pd.DatetimeIndex(df_tm_player_top5_2021['birth_date']).year\n",
    "df_tm_player_top5_2021['birth_month'] = pd.DatetimeIndex(df_tm_player_top5_2021['birth_date']).month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.5'>4.5. Columns of Interest</a>\n",
    "We are interested in the following thirteen columns in the [TransferMarkt](https://www.transfermarkt.co.uk/) dataset:\n",
    "*    `name`\n",
    "*    `name_lower`\n",
    "*    `firstinitial_lower`\n",
    "*    `firstname_lower`\n",
    "*    `lastname_lower`\n",
    "*    `position_description`\n",
    "*    `position_description_cleaned`\n",
    "*    `value`\n",
    "*    `birth_date`\n",
    "*    `birth_year`\n",
    "*    `birth_month`\n",
    "*    `age`\n",
    "*    `playing_country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "df_tm_player_top5_2021 = df_tm_player_top5_2021[['name', 'name_lower', 'firstinitial_lower', 'firstname_lower', 'lastname_lower', 'position_description', 'position_description_cleaned', 'value', 'birth_date', 'birth_year', 'birth_month', 'age', 'playing_country']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.6'>4.6. Split Dataset into Outfielder Players and Goalkeepers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign df_tm as a new DataFrame - df_tm_player_top5_all_2021_all, to represent all the players\n",
    "df_tm_player_top5_all_2021 = df_tm_player_top5_2021\n",
    "\n",
    "# Filter rows for position_description is not equal to 'Goalkeeper'\n",
    "df_tm_player_top5_outfield_2021 = df_tm_player_top5_all_2021[df_tm_player_top5_all_2021['position_description'] != 'Goalkeeper']\n",
    "\n",
    "# Filter rows for position_description are equal to 'Goalkeeper'\n",
    "df_tm_player_top5_goalkeeper_2021 = df_tm_player_top5_all_2021[df_tm_player_top5_all_2021['position_description'] == 'Goalkeeper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tm_player_top5_all_2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tm_player_top5_outfield_2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tm_player_top5_goalkeeper_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.7'>4.7. Exporting the Engineered DataFrames</a>\n",
    "Export the three engineered [TransferMarkt](https://www.transfermarkt.co.uk/) DataFrames as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define today's date\n",
    "today = datetime.datetime.now().strftime('%d/%m/%Y').replace('/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the three DataFrames\n",
    "df_tm_player_top5_all_2021.to_csv(data_dir_tm + '/engineered/all/' + f'all_big5_1920_{today}.csv', index=None, header=True)\n",
    "df_tm_player_top5_outfield_2021.to_csv(data_dir_tm + '/engineered/player/' + f'outfield_big5_1920_{today}.csv', index=None, header=True)\n",
    "df_tm_player_top5_goalkeeper_2021.to_csv(data_dir_tm + '/engineered/goalkeeper/' + f'goalkeeper_big5_1920_{today}.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created three pandas DataFrames and wrangled the data to meet our needs, we'll next conduct and [Exploratory Data Analysis ](#section5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section5'>5. Exploratory Data Analysis</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.1'>5.1. Introduction</a>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.2'>5.2. ...</a>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section6'>6. Summary</a>\n",
    "This notebook scrapes data for player valuations using [Beautifulsoup](https://pypi.org/project/beautifulsoup4/) from [TransferMarkt](https://www.transfermarkt.co.uk/) using [pandas](http://pandas.pydata.org/) for data maniuplation through DataFrames and [Beautifulsoup](https://pypi.org/project/beautifulsoup4/) for webscraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section7'>7. Next Steps</a>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section8'>8. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section9.1'>8.1. Bibliography</a>\n",
    "\n",
    "#### Data and Web Scraping\n",
    "*    [FBref](https://fbref.com/) for the data to scrape\n",
    "*    FBref statement for using StatsBomb's data: https://fbref.com/en/statsbomb/\n",
    "*    [StatsBomb](https://statsbomb.com/) providing the data to FBref\n",
    "*    [FBref_EPL GitHub repository](https://github.com/chmartin/FBref_EPL) by [chmartin](https://github.com/chmartin) for the original web scraping code\n",
    "*    [Scrape-FBref-data GitHub repository](https://github.com/parth1902/Scrape-FBref-data) by [parth1902](https://github.com/parth1902) for the revised web scraping code for the new FBref metrics\n",
    "*    [Beyond crowd judgments: Data-driven estimation of market value in association football](https://www.sciencedirect.com/science/article/pii/S0377221717304332) by Oliver MÃ¼llera, Alexander Simons, and Markus Weinmann.\n",
    "*    [06/04/2020: BBC - Premier League squads 'drop Â£1.6bn in value'](https://www.bbc.co.uk/sport/football/52221463).\n",
    "*    [tyrone_mings GitHub repository](https://github.com/FCrSTATS/tyrone_mings) by [FCrSTATS](https://github.com/FCrSTATS)\n",
    "*    [Python Package Index (PyPI) tyrone-mings library](https://pypi.org/project/tyrone-mings/).\n",
    "\n",
    "#### Countries\n",
    "*    [Comparison of alphabetic country codes Wiki](https://en.wikipedia.org/wiki/Comparison_of_alphabetic_country_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section9.2'>8.2. Python Techniques Observed</a>\n",
    "To conduct our analysis, we have used the following libraries and modules for the following tasks:\n",
    "*    [NumPy](http://www.numpy.org/) for multidimensional array computing,\n",
    "*    [pandas](http://pandas.pydata.org/) for data manipulation and ingestion, and\n",
    "*    [Beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for scraping data from webpages.\n",
    "\n",
    "We have also demonstrated an array of techniques in Python using the following methods and functions:\n",
    "*    pandas EDA methods:\n",
    "     +    [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html),\n",
    "     +    [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html),\n",
    "     +    [shape](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html),\n",
    "     +    [columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html),\n",
    "     +    [dtypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html),\n",
    "     +    [info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html), and\n",
    "     +    [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html).\n",
    "*    The [missingno](https://pypi.org/project/missingno/) library to visualise how many missing values we have in the dataset, and\n",
    "*    The pandas [.to_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) method to export the DataFrames as csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Visit my website [EddWebster.com](https://www.eddwebster.com) or my [GitHub Repository](https://github.com/eddwebster) for more projects. If you'd like to get in contact, my Twitter handle is [@eddwebster](http://www.twitter.com/eddwebster) and my email is: edd.j.webster@gmail.com.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 642,
   "position": {
    "height": "664px",
    "left": "1119px",
    "right": "20px",
    "top": "-7px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
